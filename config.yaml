preprocessing:
  target_column: Exited
  outlier_handling:
    enable: true
    iqr_multiplier: 1.5
    strategy: 'any'   # 'any' or 'all'
    max_iter: 10

feature_engineering:
  creation: true      
  transformation: true

feature_selection:
  enable: true
  method: 'correlation'   # 'correlation' | 'variance' | 'mutual_info' | 'f_score'
  threshold: 0.95       
  target_col: 'Exited' 
  k: 20                 
  percentile: null

imbalance:
  enable: true
  method: 'smote'         # 'smote' | 'adasyn' | 'smote_tomek' | 'smote_enn' | 'class_weight'
  sampling_strategy: 'auto'

split:
  test_size: 0.2
  val_size: 0.2
  random_state: 42

cv:
  n_splits: 5

training:
  enable_baselines: true
  models: ["XGBoost", "LightGBM", "CatBoost"]

tuning:
  enable: true
  n_trials: 20
  scoring: 'roc_auc'

  XGBoost:
    param_space:
      n_estimators:
        type: int
        low: 100
        high: 300
      max_depth:
        type: int
        low: 3
        high: 6
      learning_rate:
        type: float
        low: 0.03
        high: 0.1
      subsample:
        type: float
        low: 0.7
        high: 1.0
      colsample_bytree:
        type: float
        low: 0.7
        high: 1.0


  LightGBM:
    param_space:
      n_estimators:
        type: int
        values: [200, 400, 600]
      num_leaves:
        type: int
        values: [31, 63, 127]
      learning_rate:
        type: float
        values: [0.03, 0.05, 0.1]
      subsample:
        type: float
        values: [0.7, 0.8, 1.0]
      colsample_bytree:
        type: float
        values: [0.7, 0.8, 1.0]


  CatBoost:
    param_space:
      depth:
        type: int
        values: [4, 6, 8]
      learning_rate:
        type: float
        values: [0.03, 0.05, 0.1]
      iterations:
        type: int
        values: [300, 500, 800]

paths:
  combine_sources:
    - data/raw/train.csv
    - data/raw/Churn_Modelling.csv
  test_path: data/raw/test.csv
  artifacts_dir: saved_models
