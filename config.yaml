
preprocessing:
  target_column: Exited
  outlier_handling:
    enable: true
    iqr_multiplier: 1.5
    strategy: 'any'   # 'any' or 'all'
    max_iter: 10

feature_engineering:
  creation: true      
  transformation: true

feature_selection:
  enable: true
  method: 'mutual_info'   # 'correlation' | 'variance' | 'mutual_info' | 'f_score'
  threshold: 0.95       
  target_col: 'Exited' 
  k: 20                 
  percentile: null

imbalance:
  enable: true
  method: 'class_weight'         # 'smote' | 'adasyn' | 'smote_tomek' | 'smote_enn' | 'class_weight'
  sampling_strategy: 'auto'

split:
  test_size: 0.2
  val_size: 0.2
  random_state: 42

cv:
  n_splits: 5

training:
  enable_baselines: true
  models: ["XGBoost", "LightGBM", "CatBoost"]

tuning:
  enable: true
  n_iter: 20
  scoring: 'roc_auc'

  XGBoost:
    param_distributions:
      n_estimators: [100, 200, 300]
      max_depth: [3, 4, 5, 6]
      learning_rate: [0.03, 0.05, 0.1]
      subsample: [0.7, 0.8, 1.0]
      colsample_bytree: [0.7, 0.8, 1.0]
  LightGBM:
    param_distributions:
      n_estimators: [200, 400, 600]
      num_leaves: [31, 63, 127]
      learning_rate: [0.03, 0.05, 0.1]
      subsample: [0.7, 0.8, 1.0]
      colsample_bytree: [0.7, 0.8, 1.0]
  CatBoost:
    param_distributions:
      depth: [4, 6, 8]
      learning_rate: [0.03, 0.05, 0.1]
      iterations: [300, 500, 800]

paths:
  combine_sources:
    - data/raw/train.csv
    - data/raw/Churn_Modelling.csv
  test_path: data/raw/test.csv
  artifacts_dir: saved_models
